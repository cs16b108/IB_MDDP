{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IB_Diarization_14122020.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs16b108/IB_MDDP/blob/master/IB_Diarization_14122020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5jANRDmEMlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980ff21d-5656-4cd6-9ea0-a70ec55e9e45"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2xzt9uJrsZF"
      },
      "source": [
        "!pip install webrtcvad\n",
        "!pip install hmmlearn==0.2.3\n",
        "!pip install python_speech_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4zv8xRTrsZU"
      },
      "source": [
        "import os\n",
        "from os.path import isfile, isdir, join\n",
        "from pathlib import Path\n",
        "from sklearn import mixture\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from scipy.stats import multivariate_normal\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "import scipy.io.wavfile as wav\n",
        "from python_speech_features import mfcc\n",
        "import pickle\n",
        "from hmmlearn import hmm\n",
        "import time\n",
        "import copy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9V6TB8HEEy1"
      },
      "source": [
        "#File name and directory path\r\n",
        "fileName = 'ES2002c'\r\n",
        "drivePath = '/content/drive/My Drive/IB_Diarization/'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHCZqd13rsZW"
      },
      "source": [
        "#######################################################\n",
        "############# Read WAV and perform VAD ################\n",
        "# Already done once and files stored on google drive ##\n",
        "#######################################################\n",
        "\n",
        "import collections\n",
        "import contextlib\n",
        "import sys\n",
        "import wave\n",
        "import webrtcvad\n",
        "\n",
        "\n",
        "def read_wave(path):\n",
        "    \"\"\"Reads a .wav file.\n",
        "    Takes the path, and returns (PCM audio data, sample rate).\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
        "        num_channels = wf.getnchannels()\n",
        "        assert num_channels == 1\n",
        "        sample_width = wf.getsampwidth()\n",
        "        assert sample_width == 2\n",
        "        sample_rate = wf.getframerate()\n",
        "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
        "        pcm_data = wf.readframes(wf.getnframes())\n",
        "        return pcm_data, sample_rate\n",
        "\n",
        "\n",
        "def write_wave(path, audio, sample_rate):\n",
        "    \"\"\"Writes a .wav file.\n",
        "    Takes path, PCM audio data, and sample rate.\n",
        "    \"\"\"\n",
        "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
        "        wf.setnchannels(1)\n",
        "        wf.setsampwidth(2)\n",
        "        wf.setframerate(sample_rate)\n",
        "        wf.writeframes(audio)\n",
        "\n",
        "\n",
        "class Frame(object):\n",
        "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
        "    def __init__(self, bytes, timestamp, duration):\n",
        "        self.bytes = bytes\n",
        "        self.timestamp = timestamp\n",
        "        self.duration = duration\n",
        "\n",
        "\n",
        "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
        "    \"\"\"Generates audio frames from PCM audio data.\n",
        "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
        "    the sample rate.\n",
        "    Yields Frames of the requested duration.\n",
        "    \"\"\"\n",
        "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "    offset = 0\n",
        "    timestamp = 0.0\n",
        "    duration = (float(n) / sample_rate) / 2.0\n",
        "    while offset + n < len(audio):\n",
        "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
        "        timestamp += duration\n",
        "        offset += n\n",
        "\n",
        "\n",
        "def vad_collector(sample_rate, frame_duration_ms,\n",
        "                  padding_duration_ms, vad, frames,vuv_frames):\n",
        "    \"\"\"Filters out non-voiced audio frames.\n",
        "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
        "    the voiced audio.\n",
        "    Uses a padded, sliding window algorithm over the audio frames.\n",
        "    When more than 90% of the frames in the window are voiced (as\n",
        "    reported by the VAD), the collector triggers and begins yielding\n",
        "    audio frames. Then the collector waits until 90% of the frames in\n",
        "    the window are unvoiced to detrigger.\n",
        "    The window is padded at the front and back to provide a small\n",
        "    amount of silence or the beginnings/endings of speech around the\n",
        "    voiced frames.\n",
        "    Arguments:\n",
        "    sample_rate - The audio sample rate, in Hz.\n",
        "    frame_duration_ms - The frame duration in milliseconds.\n",
        "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
        "    vad - An instance of webrtcvad.Vad.\n",
        "    frames - a source of audio frames (sequence or generator).\n",
        "    Returns: A generator that yields PCM audio data.\n",
        "    \"\"\"\n",
        "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
        "    # We use a deque for our sliding window/ring buffer.\n",
        "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
        "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
        "    # NOTTRIGGERED state.\n",
        "    triggered = False\n",
        "\n",
        "    index=-1\n",
        "    voiced_frames = []\n",
        "    for frame in frames:\n",
        "\n",
        "        index+=1\n",
        "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
        "\n",
        "        sys.stdout.write('1' if is_speech else '0')\n",
        "        if not triggered:\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
        "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
        "            # the ring buffer are voiced frames, then enter the\n",
        "            # TRIGGERED state.\n",
        "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
        "                triggered = True\n",
        "                sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n",
        "                # We want to yield all the audio we see from now until\n",
        "                # we are NOTTRIGGERED, but we have to start with the\n",
        "                # audio that's already in the ring buffer.\n",
        "\n",
        "                id = 0 #we must start actually with num_padding_frames-1 and do index-- \n",
        "                for f, s in ring_buffer:\n",
        "                    voiced_frames.append(f)\n",
        "                    vuv_frames[index-id]=1\n",
        "                    id+=1\n",
        "\n",
        "                ring_buffer.clear()\n",
        "        else:\n",
        "            # We're in the TRIGGERED state, so collect the audio data\n",
        "            # and add it to the ring buffer.\n",
        "            voiced_frames.append(frame)\n",
        "            vuv_frames[index] = 1\n",
        "            ring_buffer.append((frame, is_speech))\n",
        "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
        "            # If more than 90% of the frames in the ring buffer are\n",
        "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
        "            # audio we've collected.\n",
        "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
        "                sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
        "                triggered = False\n",
        "                yield b''.join([f.bytes for f in voiced_frames])\n",
        "                ring_buffer.clear()\n",
        "                voiced_frames = []\n",
        "    if triggered:\n",
        "        sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
        "    sys.stdout.write('\\n')\n",
        "    # If we have any leftover voiced audio when we run out of input,\n",
        "    # yield it.\n",
        "    if voiced_frames:\n",
        "        yield b''.join([f.bytes for f in voiced_frames])\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na0eIG-qrsZY"
      },
      "source": [
        "def main():\n",
        "    # if len(args) != 2:\n",
        "    #     sys.stderr.write(\n",
        "    #         'Usage: silenceremove.py <aggressiveness> <path to wav file>\\n')\n",
        "    #     sys.exit(1)\n",
        "    # path = '/content/amicorpus/'\n",
        "    path = join(drivePath, 'temp/'+fileName)\n",
        "    dir_list = sorted(os.listdir(path))\n",
        "    cnt = 0\n",
        "    for d in dir_list:\n",
        "      dir_name = join(path,d)\n",
        "      if not isdir(dir_name):\n",
        "        # filePath = join(path, d, 'audio', d+'.Mix-Headset.wav')\n",
        "        filePath = join(path, d)\n",
        "        # try:\n",
        "        # if 1:\n",
        "        audio, sample_rate = read_wave(filePath)\n",
        "        vad = webrtcvad.Vad(int(1))\n",
        "        frame_duration_ms = 30\n",
        "        frames = frame_generator(30, audio, sample_rate)\n",
        "        frames = list(frames)\n",
        "\n",
        "        nof_frames = 1+(len(audio)-1)/int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
        "        nof_frames = int(nof_frames)\n",
        "        vuv_frames = np.zeros((nof_frames,)).astype('int')\n",
        "        segments = vad_collector(sample_rate, 30, 300, vad, frames,vuv_frames)\n",
        "        print(len(frames),\" s\", sample_rate,len(audio))\n",
        "\n",
        "        # Segmenting the Voice audio and save it in list as bytes\n",
        "        concataudio = [segment for segment in segments]\n",
        "\n",
        "        joinedaudio = b\"\".join(concataudio)\n",
        "        np.save(join(drivePath,'save_files', fileName+'_vuv.npy'),vuv_frames)\n",
        "        # writePath = join('/content/drive/My Drive/IB_Diarization/temp/conv', d)\n",
        "        # Path(writePath).mkdir(parents=True, exist_ok=True)\n",
        "        # write_wave(writePath, joinedaudio, sample_rate)\n",
        "        cnt += 1\n",
        "        if(cnt == 2):\n",
        "          break\n",
        "        # except Exception as inst:\n",
        "          # print(\"Skipping: \", filePath, inst)\n",
        "    print(\"Converted: \",cnt)\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eK9jDzPspJZ"
      },
      "source": [
        "vuv_frames = np.load(join(drivePath,'save_files', fileName+'_vuv.npy'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZePchLWDrsZx"
      },
      "source": [
        "####################################\n",
        "### Actual Code Starts from Here ###\n",
        "####################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ekr76KGrsZ0"
      },
      "source": [
        "#Define path to wav files created after VAD\n",
        "\n",
        "path = join(drivePath, 'amicorpus_non_silence/')\n",
        "\n",
        "overlap = 0.01 #10 ms window shift\n",
        "fullPath = join(path,fileName+'/audio/'+fileName+'.Mix-Headset.wav')\n",
        "(rate,sig) = wav.read(fullPath)\n",
        "mfcc_feat = mfcc(sig, rate, numcep = 19, nfilt = 26, winlen=0.03, winstep=overlap)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA8PWs8UgExB"
      },
      "source": [
        "#Also read the original wav file\r\n",
        "path = join(drivePath, 'temp/'+fileName)\r\n",
        "fullPath = join(path,fileName+'.Mix-Headset.wav')\r\n",
        "(rate,sig) = wav.read(fullPath)\r\n",
        "mfcc_feat_original = mfcc(sig, rate, numcep = 19, nfilt = 26, winlen=0.03, winstep=overlap)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA4XNF2XxeAx"
      },
      "source": [
        "##n is total num of frames and d is num of features per frame\r\n",
        "n, d = mfcc_feat.shape"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYpv2WHys4m6"
      },
      "source": [
        "# overlap = 0.01 #10 ms window shift\n",
        "init_cluster_time = 2500 #2.5sec\n",
        "init_cluster_len = math.ceil(init_cluster_time/(overlap*1000))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fttKNb7UtQ7z"
      },
      "source": [
        "N = math.ceil(n/init_cluster_len)\n",
        "num_of_clusters = N"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_AYZWAdxN_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "755164dd-d804-4515-fa70-22cdfab67233"
      },
      "source": [
        "print(N,n,init_cluster_len, (n/init_cluster_len) )"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "879 219619 250 878.476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2qzsMekx04_"
      },
      "source": [
        "# class GMM:\n",
        "#     def __init__(self, num_of_clusters):\n",
        "#         self.num_of_clusters = num_of_clusters\n",
        "#         self.log_likelihood =[]\n",
        "#         self.LL_diff = []\n",
        "#         # self.num_of_speakers = num_of_speakers\n",
        "\n",
        "#     def gaussian_prob(self, x, mean, sigma):\n",
        "#         d = x.shape[0]\n",
        "#         p = ((2*math.pi)**(-d/2))*(np.linalg.det(sigma)**(-0.5))*np.exp(-0.5*(x-mean).reshape(d,1).T.dot(np.linalg.inv(sigma)).dot((x-mean).reshape(d,1)))\n",
        "#         return p\n",
        "\n",
        "#     def k_means(self, X):\n",
        "#         n = X.shape[0]\n",
        "#         d = X.shape[1]\n",
        "#         itr = 0\n",
        "#         #self.centroid = np.zeros((self.num_of_clusters, d), dtype = 'float64')\n",
        "#         self.centroids = X[random.sample(range(n), self.num_of_clusters)]\n",
        "#         self.cluster_assigned = np.zeros(n, dtype = int)\n",
        "#         error = 0.0\n",
        "#         while True:\n",
        "#             print(\"Now at itr - \", itr)\n",
        "#             # print(\"Centroids - \", self.centroids)\n",
        "#             for i in range(n):\n",
        "#                 f_vec = X[i]\n",
        "#                 dist = np.sqrt(np.sum((f_vec-self.centroids)**2, 1))\n",
        "#                 # print(\"Dist Shape is - \", dist.shape)\n",
        "#                 self.cluster_assigned[i] = np.argmin(dist)\n",
        "#             new_error = np.sum(np.sqrt(np.sum((X - self.centroids[self.cluster_assigned])**2, 1)))\n",
        "#             if(itr>0):\n",
        "#                 print(\"Error Difference is - \", np.abs(error-new_error))\n",
        "#             new_centroids = np.zeros((self.num_of_clusters, d), dtype = 'float64')\n",
        "#             count_of_elements = np.zeros(self.num_of_clusters, dtype = int)\n",
        "#             for i in range(n):\n",
        "#                 c_ind = self.cluster_assigned[i]\n",
        "#                 new_centroids[c_ind] += X[i]\n",
        "#                 count_of_elements[c_ind] += 1\n",
        "#             new_centroids = new_centroids/count_of_elements[:,None]\n",
        "#             if np.abs(new_error-error)<10 or np.array_equal(self.centroids, new_centroids) or itr>=5:\n",
        "#                 print(\"Breaking at itr - \", itr)\n",
        "#                 break\n",
        "#             else:\n",
        "#                 self.centroids = np.copy(new_centroids)\n",
        "#             itr += 1\n",
        "#             error = new_error\n",
        "\n",
        "#     def EM_GMM_INBUILT(self, X):\n",
        "#         N = X.shape[0]\n",
        "#         d = X.shape[1]\n",
        "#         from sklearn.mixture import GaussianMixture as GMM\n",
        "#         g = GMM(n_components=64, covariance_type = 'full', max_iter = 1)\n",
        "#         g.fit(X)\n",
        "#         print(\"Created\")\n",
        "\n",
        "#     def EM_GMM(self, X):\n",
        "#         N = X.shape[0]\n",
        "#         d = X.shape[1]\n",
        "#         self.cov_mat = np.zeros((self.num_of_clusters, d, d), dtype = 'float64')\n",
        "#         self.gamma = np.zeros((N,self.num_of_clusters), dtype = 'float64')\n",
        "#         likelihood = np.zeros((N,self.num_of_clusters), dtype = 'float64')\n",
        "#         self.pi_prob = np.zeros(self.num_of_clusters, dtype = 'float64')\n",
        "#         self.Nk = np.zeros(self.num_of_clusters, dtype = 'float64')\n",
        "#         for k in range(self.num_of_clusters):\n",
        "#             indices = (np.argwhere(self.cluster_assigned==k)).ravel()\n",
        "#             X_k = X[indices]\n",
        "#             X_k_centered = X_k - self.centroids[k]\n",
        "#             self.Nk[k] = X_k.shape[0]\n",
        "#             # print(\"Xk \",X_k.shape)\n",
        "#             # print(\"Xkc \",X_k_centered.shape)\n",
        "#             # print(\"cov mat \",self.cov_mat[k])\n",
        "#             self.cov_mat[k] = (1/self.Nk[k])*(X_k_centered.T.dot(X_k_centered))\n",
        "#         # print(self.Nk)\n",
        "#         self.pi_prob = self.Nk/N\n",
        "#         print(\"EM Begins\")\n",
        "#         itr = 1\n",
        "#         prev_log_likelihood = 0.0\n",
        "        \n",
        "#         while True:\n",
        "#             #####################################\n",
        "#             ############   E Step   #############\n",
        "#             #####################################\n",
        "#             for k in range(self.num_of_clusters):\n",
        "#                 #self.gamma[i,k] = self.gaussian_prob(X[i], self.centroids[k], self.cov_mat[k])\n",
        "#                 self.cov_mat[k] += 1e-6*np.identity(d)\n",
        "#                 likelihood[:,k] =  multivariate_normal.pdf(X, self.centroids[k], self.cov_mat[k]).ravel()\n",
        "#                 # print(\"Done \", k)\n",
        "#             # log_likelihood = np.sum(np.sum((likelihood*self.pi_prob), axis = 1))\n",
        "\n",
        "#             # for i in range(N):\n",
        "#             #     print(\"Done \",i)\n",
        "             \n",
        "#             self.gamma = likelihood*self.pi_prob\n",
        "#             self.gamma = self.gamma/(np.sum(self.gamma, axis = 1)[:,None])\n",
        "#             # print(\"E done\")\n",
        "\n",
        "#             #####################################\n",
        "#             ############   M Step   #############\n",
        "#             #####################################\n",
        "#             self.Nk = np.sum(self.gamma, axis = 0)\n",
        "#             self.pi_prob = self.Nk/N\n",
        "#             for k in range(self.num_of_clusters):\n",
        "#                 self.centroids[k] = (1/self.Nk[k])*np.sum((X*self.gamma[:,k][:,np.newaxis]), axis = 0)\n",
        "#                 X_centered = X - self.centroids[k]\n",
        "#                 self.cov_mat[k] = (1/self.Nk[k])*((X_centered*self.gamma[:,k][:,np.newaxis]).T.dot(X_centered))\n",
        "#             # print(\"M done\")\n",
        "\n",
        "#             #####################################\n",
        "#             ########   Log Likelihood   #########\n",
        "#             #####################################\n",
        "#             new_log_likelihood = np.sum(np.log(np.sum((likelihood*self.pi_prob), axis = 1)))\n",
        "#             self.log_likelihood.append(new_log_likelihood)\n",
        "#             diff_LL = np.abs(new_log_likelihood-prev_log_likelihood)\n",
        "#             self.LL_diff.append(diff_LL)\n",
        "#             print(\"Itr = \", itr, \" Current LL is - \",new_log_likelihood)\n",
        "#             print(\"Change In LL is - \",diff_LL)\n",
        "#             if(diff_LL<100 or itr>=10):\n",
        "#                 print(\"EM Finished at iteration - \", itr)\n",
        "#                 break\n",
        "#             itr += 1\n",
        "#             prev_log_likelihood = new_log_likelihood\n",
        "\n",
        "# ug = GMM(num_of_clusters)\n",
        "# ug.k_means(mfcc_feat)\n",
        "# ug.EM_GMM(mfcc_feat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihYpZGpIY5WX"
      },
      "source": [
        "def fitUnimodal(C):\n",
        "  means = []\n",
        "  covMatrices = []\n",
        "  for c in C:\n",
        "    means.append(np.mean(c, axis = 0))\n",
        "    covMatrices.append(np.cov(c.T))\n",
        "  return means, covMatrices"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O54Lqzff0zce"
      },
      "source": [
        "def calc_prob(x, GaussianMeans, GaussianCovMatrices):\n",
        "  p = 0.0\n",
        "  D = x.shape[0]\n",
        "  numOfClusters = len(GaussianMeans)\n",
        "  for i in range(D):\n",
        "    s = x[i]\n",
        "    for k in range(numOfClusters):\n",
        "    #self.gamma[i,k] = self.gaussian_prob(X[i], self.centroids[k], self.cov_mat[k])\n",
        "      cov_matrix = 1e-6*np.identity(d) + GaussianCovMatrices[k]\n",
        "      # cov_matrix = \n",
        "      p =  p + ug.pi_prob[k]*multivariate_normal.pdf(s, ug.centroids[k], cov_matrix)\n",
        "  p = p/D\n",
        "  return p"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y7msXyWJkMH"
      },
      "source": [
        "def calcYgivenX(x, GaussianMeans, GaussianCovMatrices, i):\n",
        "  p = 0.0\n",
        "  numOfClusters = len(GaussianMeans)\n",
        "  w = 1.0/numOfClusters\n",
        "  D = x.shape[0]\n",
        "  probMat = np.zeros((D, num_of_clusters), dtype = float)\n",
        "  for i in range(num_of_clusters):\n",
        "    probMat[:,i] = multivariate_normal(x, GaussianMeans[i], GaussianCovMatrices[i])\n",
        "  p = 0.0\n",
        "  self.gamma = self.gamma/(np.sum(self.gamma, axis = 1)[:,None]) \n",
        "  return p"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUZLV2W5RKZo"
      },
      "source": [
        "########################\n",
        "##### IB Algorithm #####\n",
        "########################\n",
        "\n",
        "#Init Variables\n",
        "C = np.array_split(mfcc_feat, num_of_clusters)\n",
        "GaussianMeans, GaussianCovMatrices = fitUnimodal(C)\n",
        "ClusterMapping = dict(zip(range(num_of_clusters), [[i] for i in range(num_of_clusters)]))\n",
        "beta = 10.0"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zDI3qIlQHei"
      },
      "source": [
        "# probC = []\n",
        "# for i in range(N):\n",
        "#   p = 0.0\n",
        "#   D = C[i].shape[0]\n",
        "#   for j in range(D):\n",
        "#     s = C[i][j]\n",
        "#     p += multivariate_normal.pdf(s, GaussianMeans[i], GaussianCovMatrices[i])\n",
        "#   p = p/D \n",
        "#   probC.append(p)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQejbER_OfF9"
      },
      "source": [
        "probC = (1.0/N)*np.ones(N)\n",
        "probX = probC.copy()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfyeSjbwyHgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bea2464-8da2-49aa-ec6e-e48ec2e7a410"
      },
      "source": [
        "probYgivenC = []\n",
        "probCgivenX = []\n",
        "for i in range(N):\n",
        "  temp1 = []\n",
        "  temp2 = []\n",
        "  x = C[i]\n",
        "  w = 1.0/N\n",
        "  D = x.shape[0]\n",
        "  probMat = np.zeros((D, N), dtype = float)\n",
        "  for j in range(N):\n",
        "    probMat[:,j] = multivariate_normal.pdf(x, GaussianMeans[j], GaussianCovMatrices[j]).ravel()\n",
        "  probMat = probMat/(np.sum(probMat, axis = 1)[:,None])\n",
        "  temp1 = np.mean(probMat, axis = 0)\n",
        "  for j in range(N):\n",
        "    # p = probMat[i,j]/(np.sum(probMat[i,j], axis = 1)[:,None])\n",
        "    # p = calcYgivenX(x, GaussianMeans, GaussianCovMatrices, i)\n",
        "    # temp1.append(p)\n",
        "    if j == i:\n",
        "      temp2.append(1.0)\n",
        "    else:\n",
        "      temp2.append(0.0)\n",
        "    # print(\"Done2 \",j)\n",
        "  probYgivenC.append(temp1)\n",
        "  probCgivenX.append(temp2)\n",
        "  if i%100 == 0:\n",
        "    print(\"Done \",i)\n",
        "\n",
        "# # prob_cond_y_c = np.zeros((N, N), dtype = float)\n",
        "# # prob_cond_c_x = np.zeros((N, N), dtype = float)\n",
        "# del_F = np.zeros((N, N), dtype = float)\n",
        "# for i in range(N):\n",
        "#   prob_c(i) = calc_prob(C[i], ug)\n",
        "#   for j in range(N):\n",
        "#     prob_cond_y_c[j][i] = calc_cond_prob(j, C[i], ug)\n",
        "#     if(j == i):\n",
        "#       prob_cond_c_x[j][i] = 1\n",
        "\n",
        "\n",
        "\n",
        "#Main Algo\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done  0\n",
            "Done  100\n",
            "Done  200\n",
            "Done  300\n",
            "Done  400\n",
            "Done  500\n",
            "Done  600\n",
            "Done  700\n",
            "Done  800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy1GaieALLwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da066ee-3bda-42bb-86b7-6206240b2edc"
      },
      "source": [
        "del_F = np.zeros((N, N), dtype = float)\n",
        "del_F[:,:] = np.inf\n",
        "probXgivenC = ((np.array(probCgivenX)*np.array(probX)).T/probC).T\n",
        "for i in range(N):\n",
        "  for j in range(i+1, N): \n",
        "    temp1 = distance.jensenshannon(np.array(probYgivenC)[:,i], np.array(probYgivenC)[:,j]) \n",
        "    temp2 = distance.jensenshannon(probXgivenC[i], probXgivenC[j]) \n",
        "    dij = temp1 - (1/beta)*temp2\n",
        "    del_F[i][j] = (probC[i] + probC[j])*dij\n",
        "    # del_F[i][j] = cal_objective_diff(C[i], C[j])\n",
        "  if i%100 == 0:\n",
        "    print(\"Done \",i)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done  0\n",
            "Done  100\n",
            "Done  200\n",
            "Done  300\n",
            "Done  400\n",
            "Done  500\n",
            "Done  600\n",
            "Done  700\n",
            "Done  800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UBnkJ0-Bgl5"
      },
      "source": [
        "import pickle\n",
        "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probYgivenC.sav'\n",
        "pickle.dump(probYgivenC, open(file_name, 'wb'))\n",
        "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probCgivenX.sav'\n",
        "pickle.dump(probCgivenX, open(file_name, 'wb'))\n",
        "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_del_F.sav'\n",
        "pickle.dump(del_F, open(file_name, 'wb'))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcZYLCvL-By"
      },
      "source": [
        "import pickle\n",
        "probC = (1.0/N)*np.ones(N)\n",
        "probX = probC.copy()\n",
        "ClusterMapping = dict(zip(range(N), [[i] for i in range(N)]))\n",
        "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probYgivenC.sav'\n",
        "probYgivenC = pickle.load(open(file_name, 'rb'))\n",
        "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probCgivenX.sav'\n",
        "# pickle.dump(probCgivenY, open(file_name, 'wb'))\n",
        "probCgivenX = pickle.load(open(file_name, 'rb'))\n",
        "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_del_F.sav'\n",
        "del_F = pickle.load(open(file_name, 'rb'))"
      ],
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWu50k8pX0bb"
      },
      "source": [
        "num_of_speakers = 4\n",
        "hmm_gmm_cluster_num = 5"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwZKVQEKHncy"
      },
      "source": [
        "# Nif num_of_clusters == 4:\r\n",
        "bestClusterMapping = copy.deepcopy(ClusterMapping)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uqki82tSBf0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6406c4a8-197e-4f42-c00e-52ba7e89dc38"
      },
      "source": [
        "#IB ALgo\n",
        "import time\n",
        "startTime = time.time()\n",
        "num_of_clusters = N\n",
        "# print(\"Yaha\")\n",
        "\n",
        "INFyc = 0.0\n",
        "probY = 1.0/N\n",
        "for y in range(N):\n",
        "  for c in range(num_of_clusters):\n",
        "    INFyc += probC[c]*probYgivenC[y][c]*np.log(probYgivenC[y][c]/probY)\n",
        "\n",
        "INFyx = INFyc\n",
        "NMI = INFyc/INFyx\n",
        "threshold = 0.5\n",
        "while num_of_clusters > 1:\n",
        "  # print(\"Here\")\n",
        "  # i, j = np.argwhere(del_F == np.min(del_F)).ravel()\n",
        "  mIdx = np.argmin(del_F)\n",
        "  i = mIdx//N\n",
        "  j = mIdx%N\n",
        "  # print(i,j)\n",
        "  probCr = probC[i] + probC[j]\n",
        "  del_F[:,j] = np.inf\n",
        "  del_F[j,:] = np.inf\n",
        "  # probC.pop(j)\n",
        "  ClusterMapping[i] += ClusterMapping[j]\n",
        "  ClusterMapping[j] = []\n",
        "  probYgivenC[i] = (probYgivenC[i]*probC[i] + probYgivenC[j]*probC[j])/probCr\n",
        "  probC[i] = probCr\n",
        "  probCgivenX[i] = [0 for idx in probCgivenX[i]]\n",
        "  for idx in ClusterMapping[i]:\n",
        "    probCgivenX[i][idx] = 1\n",
        "  probXgivenC = ((np.array(probCgivenX)*np.array(probX)).T/probC).T\n",
        "  for idx in range(0, i):\n",
        "    if del_F[idx,i] == np.inf:\n",
        "      continue\n",
        "    temp1 = distance.jensenshannon(np.array(probYgivenC)[:,idx], np.array(probYgivenC)[:,i]) \n",
        "    temp2 = distance.jensenshannon(probXgivenC[idx], probXgivenC[i]) \n",
        "    dij = temp1 - (1/beta)*temp2\n",
        "    del_F[idx][i] = (probC[idx] + probC[i])*dij\n",
        "  for idx in range(i+1, N):\n",
        "    if del_F[i, idx] == np.inf:\n",
        "      continue \n",
        "    temp1 = distance.jensenshannon(np.array(probYgivenC)[:,i], np.array(probYgivenC)[:,idx]) \n",
        "    temp2 = distance.jensenshannon(probXgivenC[i], probXgivenC[idx]) \n",
        "    dij = temp1 - (1/beta)*temp2\n",
        "    del_F[i][idx] = (probC[i] + probC[idx])*dij\n",
        "  num_of_clusters = num_of_clusters-1\n",
        "  \n",
        "  if num_of_clusters == num_of_speakers:\n",
        "    print(\"Deep Copying dict:\")\n",
        "    bestClusterMapping = copy.deepcopy(ClusterMapping)\n",
        "\n",
        "  INFyc = 0.0\n",
        "  for y in range(N):\n",
        "    for c in range(N):\n",
        "      if ClusterMapping[c]:\n",
        "        INFyc += probC[c]*probYgivenC[y][c]*np.log(probYgivenC[y][c]/probY)\n",
        "  NMI = INFyc/INFyx\n",
        "  if num_of_clusters%50 == 0 or num_of_clusters<=10:\n",
        "    # print(i,j)\n",
        "    # cnt = 0\n",
        "    # for idx in range(N):\n",
        "    #   if len(ClusterMapping[idx]) !=0:\n",
        "    #     cnt += len(ClusterMapping[idx])\n",
        "    #     if num_of_clusters<10:\n",
        "    #       print(idx, len(ClusterMapping[idx]))\n",
        "    # print(\"\")\n",
        "    # for idx in range(N):\n",
        "    #   if len(bestClusterMapping[idx]) !=0:\n",
        "    #     if num_of_clusters<6:\n",
        "    #       print(idx, len(bestClusterMapping[idx]))\n",
        "    print(\"All Count: \",cnt)\n",
        "    print(\"Clusters Rem: \", num_of_clusters)\n",
        "    print(\"NMI: \",NMI)\n",
        "    print(\"Time Elapsed: \", (time.time()-startTime)/60, \" minutes\")\n",
        "\n",
        "print(\"Completion Time: \", (time.time()-startTime)/60, \" minutes\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Count:  76\n",
            "Clusters Rem:  850\n",
            "NMI:  0.9760661359244392\n",
            "Time Elapsed:  2.810843535264333  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  800\n",
            "NMI:  0.9438554604666591\n",
            "Time Elapsed:  7.338755683104197  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  750\n",
            "NMI:  0.9162858771072044\n",
            "Time Elapsed:  11.577003220717112  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  700\n",
            "NMI:  0.8935194559072733\n",
            "Time Elapsed:  15.579637630780537  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  650\n",
            "NMI:  0.8594555734149203\n",
            "Time Elapsed:  19.271832331021628  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  600\n",
            "NMI:  0.8136385116507342\n",
            "Time Elapsed:  22.708418889840445  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  550\n",
            "NMI:  0.7561084391384516\n",
            "Time Elapsed:  25.899140242735545  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  500\n",
            "NMI:  0.7066188999334495\n",
            "Time Elapsed:  28.849774038791658  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  450\n",
            "NMI:  0.6330973827302447\n",
            "Time Elapsed:  31.52183584769567  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  400\n",
            "NMI:  0.589260819130265\n",
            "Time Elapsed:  33.95722486972809  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  350\n",
            "NMI:  0.5629544667807936\n",
            "Time Elapsed:  36.08588266770045  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  300\n",
            "NMI:  0.5109057884459016\n",
            "Time Elapsed:  37.94836415847143  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  250\n",
            "NMI:  0.4494478921969443\n",
            "Time Elapsed:  39.554379034042356  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  200\n",
            "NMI:  0.41257623886972117\n",
            "Time Elapsed:  40.89012269576391  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  150\n",
            "NMI:  0.37200089643631895\n",
            "Time Elapsed:  41.95282102028529  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  100\n",
            "NMI:  0.3396312244253798\n",
            "Time Elapsed:  42.76322103738785  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  50\n",
            "NMI:  0.2906383474418636\n",
            "Time Elapsed:  43.31389003594716  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  10\n",
            "NMI:  0.5617480749851617\n",
            "Time Elapsed:  43.53918634255727  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  9\n",
            "NMI:  0.5287250956031496\n",
            "Time Elapsed:  43.542074131965634  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  8\n",
            "NMI:  0.5117105488962339\n",
            "Time Elapsed:  43.545231544971465  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  7\n",
            "NMI:  0.5493949972303214\n",
            "Time Elapsed:  43.54841907024384  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  6\n",
            "NMI:  0.6499820743174654\n",
            "Time Elapsed:  43.551803437868756  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  5\n",
            "NMI:  0.6303179976870308\n",
            "Time Elapsed:  43.55487885872523  minutes\n",
            "Deep Copying dict:\n",
            "All Count:  76\n",
            "Clusters Rem:  4\n",
            "NMI:  0.37270436188789047\n",
            "Time Elapsed:  43.5579486767451  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  3\n",
            "NMI:  0.3428552540182747\n",
            "Time Elapsed:  43.56085183620453  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  2\n",
            "NMI:  0.7751342319976668\n",
            "Time Elapsed:  43.56364560524623  minutes\n",
            "All Count:  76\n",
            "Clusters Rem:  1\n",
            "NMI:  0.0731778883346223\n",
            "Time Elapsed:  43.56640756527583  minutes\n",
            "Completion Time:  43.56641583442688  minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph31cevADnKY"
      },
      "source": [
        "ClusterMapping = copy.deepcopy(bestClusterMapping)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcZ-Ojw98Y07"
      },
      "source": [
        "#Save the results of IB Diarization\n",
        "file_name = join(drivePath,fileName+'done.sav')\n",
        "doneFile = [N, probX, probC, probYgivenC, probCgivenX, probXgivenC, ClusterMapping, GaussianMeans, GaussianCovMatrices]\n",
        "pickle.dump(doneFile, open(file_name, 'wb'))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBkjL9mQp_vy"
      },
      "source": [
        "#Load the results of IB Diarization\n",
        "file_name = join(drivePath,fileName+'done.sav')\n",
        "doneFile = pickle.load(open(file_name, 'rb'))\n",
        "N = doneFile[0]\n",
        "probX= doneFile[1]\n",
        "probC= doneFile[2]\n",
        "probYgivenC= doneFile[3]\n",
        "probCgivenX= doneFile[4]\n",
        "probXgivenC= doneFile[5]\n",
        "ClusterMapping = doneFile[6]\n",
        "GaussianMeans = doneFile[7]\n",
        "GaussianCovMatrices = doneFile[8]"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS310aOYdwD0"
      },
      "source": [
        "###################################\r\n",
        "#### HMM Alignment Begins Here ####\r\n",
        "###################################"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omADzRjJRWzc"
      },
      "source": [
        "#Segregate MFCCs for each cluster for creating GMMs for HMM\n",
        "segCount = 0\n",
        "mfcc_segregated = []\n",
        "for key in ClusterMapping:\n",
        "  if(len(ClusterMapping[key])>0):\n",
        "    cur_mfcc = np.empty((0,19), dtype = float)\n",
        "    for curSeg in sorted(ClusterMapping[key]):\n",
        "      sIdx = max(0, (curSeg)*init_cluster_len)\n",
        "      eIdx = min((curSeg+1)*init_cluster_len, n)\n",
        "      cur_mfcc = np.concatenate((cur_mfcc, np.array(mfcc_feat[sIdx:eIdx])))\n",
        "    mfcc_segregated.append(cur_mfcc)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhY3ukDqqH1g"
      },
      "source": [
        "#Calculate parameters for initializing HMM\n",
        "gmmMeans = np.zeros((num_of_speakers, hmm_gmm_cluster_num,mfcc_feat.shape[-1]), dtype = float)\n",
        "gmmCov = np.zeros((num_of_speakers, hmm_gmm_cluster_num ,mfcc_feat.shape[-1], mfcc_feat.shape[-1]), dtype = float)\n",
        "gmmWeights = np.zeros((num_of_speakers, hmm_gmm_cluster_num ), dtype = float)\n",
        "for i, X in enumerate(mfcc_segregated):\n",
        "  gmm = mixture.GaussianMixture(n_components=hmm_gmm_cluster_num).fit(X)\n",
        "  gmmMeans[i] = gmm.means_\n",
        "  gmmCov[i] = gmm.covariances_\n",
        "  gmmWeights[i] = gmm.weights_"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yazhSYH5HCbl"
      },
      "source": [
        "#Save GMM results\n",
        "file_name = join(drivePath, fileName+'_gmmTrained.sav')\n",
        "gmmTrained = [gmmMeans, gmmCov, gmmWeights]\n",
        "pickle.dump(gmmTrained, open(file_name, 'wb'))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQ2e6foNTZx1"
      },
      "source": [
        "#Load GMM results\n",
        "file_name = join(drivePath, fileName+'_gmmTrained.sav')\n",
        "gmmTrained = pickle.load(open(file_name, 'rb'))\n",
        "gmmMeans = gmmTrained[0]\n",
        "gmmCov = gmmTrained[1]\n",
        "gmmWeights = gmmTrained[2]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXMQd4gimSjA"
      },
      "source": [
        "#Separate the final clusters into a list\n",
        "cluster_list = []\n",
        "for i in range(N):\n",
        "  if len(ClusterMapping[i]) !=0:\n",
        "    cluster_list.append(ClusterMapping[i]);\n",
        "\n",
        "#Map Which cluster belongs to which speaker\n",
        "cluster_to_speaker = np.ones((N,))\n",
        "for sp_id, clstr in enumerate(cluster_list):\n",
        "  for c in clstr:\n",
        "    cluster_to_speaker[c] = int(sp_id)\n",
        "\n",
        "#Start probability for HMM\n",
        "start_prob=np.zeros((num_of_speakers,))\n",
        "start_prob[int(cluster_to_speaker[0])] = 1.0"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4P0fxm9x0k3"
      },
      "source": [
        "#Calculate Transmission Probability for HMM\n",
        "transmission_prob = np.zeros((num_of_speakers,num_of_speakers), dtype = float)\n",
        "for i in range(1,N):\n",
        "  fromSpeaker = int(cluster_to_speaker[i-1])\n",
        "  toSpeaker = int(cluster_to_speaker[i])\n",
        "  transmission_prob[fromSpeaker][toSpeaker] += 1\n",
        "for i in range(num_of_speakers):\n",
        "  total = np.sum(cluster_to_speaker[:-1] == i)\n",
        "  transmission_prob[i,:] /= total"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gdLW_Gu3hik"
      },
      "source": [
        "# Initializie HMM\n",
        "U_GMM_HMM =  hmm.GMMHMM(n_components = num_of_speakers,\n",
        "                        n_mix = hmm_gmm_cluster_num,\n",
        "                        covariance_type = \"full\",\n",
        "                        init_params = \"\",\n",
        "                        n_iter = 50\n",
        "                         )\n",
        "U_GMM_HMM.covars_ = gmmCov\n",
        "U_GMM_HMM.means_ = gmmMeans\n",
        "U_GMM_HMM.weights_ = gmmWeights\n",
        "U_GMM_HMM.transmat_ = transmission_prob\n",
        "U_GMM_HMM.startprob_ = start_prob"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrXZwq0M0kCc"
      },
      "source": [
        "viterbiAligned = U_GMM_HMM.predict(mfcc_feat_original)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10hQKhx19Z49"
      },
      "source": [
        "#Mask each silence frame to value num_of_speakers+1\r\n",
        "for i in range(len(viterbiAligned)):\r\n",
        "  if vuv_frames[i//3] == 0:\r\n",
        "    viterbiAligned[i] = num_of_speakers+1"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgqxt-Rp94dR"
      },
      "source": [
        "def most_common(lst):\n",
        "    return max(set(lst), key=lst.count)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__OHYyJnORyA"
      },
      "source": [
        "# finalSegments = []\n",
        "# for i in range(N):\n",
        "#   temp = viterbiAlignedSpeakers[i*init_cluster_len:min((i+1)*init_cluster_len, len(viterbiAlignedSpeakers))]\n",
        "#   x = np.bincount(temp).argmax()\n",
        "#   finalSegments.append(x)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9hGXt9h95gv"
      },
      "source": [
        "#Generate final segments after viterbi alignment\r\n",
        "finalSegments = []\r\n",
        "init_cluster_timee = 2500 #2.5sec, is the min speaker segment size\r\n",
        "init_cluster_lenn = math.ceil(init_cluster_timee/(overlap*1000))\r\n",
        "NN = math.ceil(n/init_cluster_lenn)\r\n",
        "for i in range(NN):\r\n",
        "  temp = viterbiAligned[i*init_cluster_lenn:min((i+1)*init_cluster_lenn, len(viterbiAligned))]\r\n",
        "  x = np.bincount(temp).argmax()\r\n",
        "  finalSegments.append(x)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnS3YgMHNU1H"
      },
      "source": [
        "#Generate RTTM file for predicted data\n",
        "def segmentToRTTM(finalSegments, init_cluster_time):\n",
        "  fileObj = open(fileName+'_predicted_2500.rttm', 'a')\n",
        "  idx = 0\n",
        "  startTime = -1\n",
        "  duration = 0\n",
        "  # speakerId\n",
        "  recName = fileName\n",
        "  channelId = '1'\n",
        "  onset = 0.000\n",
        "  duration = 0.0\n",
        "  while idx<=NN:\n",
        "    if idx == NN or finalSegments[idx] != finalSegments[idx-1]:\n",
        "      lineStr = 'SPEAKER ' + 'meeting' + ' ' + channelId + ' ' + str(onset) + ' ' + str(duration) + ' <NA> <NA> ' \\\n",
        "                    + 'speaker_'+str(int(finalSegments[idx-1])) + ' <NA> <NA>\\n'\n",
        "      if finalSegments[idx-1] != num_of_speakers+1 and duration>0.0:\n",
        "        fileObj.writelines(lineStr)\n",
        "      onset += duration\n",
        "      duration = init_cluster_time/1000\n",
        "    else:\n",
        "      duration += init_cluster_time/1000\n",
        "    idx += 1\n",
        "  fileObj.close()\n",
        "try:\n",
        "    os.remove(fileName+'_predicted_2500.rttm')\n",
        "except OSError:\n",
        "    pass\n",
        "segmentToRTTM(finalSegments, init_cluster_timee)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT1Y459bC_h7"
      },
      "source": [
        "# f = open('actual.rttm', 'r')\r\n",
        "# fr = open('new_actual.rttm','a')\r\n",
        "# lastEnd = 0.0\r\n",
        "# for l in f.readlines():\r\n",
        "#   lst = l.split()\r\n",
        "#   lineStr = 'SPEAKER ' + 'meeting' + ' ' + '1' + ' ' + str(lastEnd) + ' ' + lst[4] + ' <NA> <NA> ' \\\r\n",
        "#                     + lst[7] + ' <NA> <NA>\\n'\r\n",
        "#   # lst[3] = str(lastEnd)\r\n",
        "#   lastEnd = lastEnd + float(lst[4])\r\n",
        "#   fr.writelines(lineStr)\r\n",
        "#   # break\r\n",
        "# f.close()\r\n",
        "# fr.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw_EMmdWwDm9"
      },
      "source": [
        "# f = open(fileName+'.scp', 'a')\r\n",
        "# l = len(vuv_frames)\r\n",
        "# i = 0\r\n",
        "# while i<l:\r\n",
        "#   if vuv_frames[i] == 1:\r\n",
        "#     j = i\r\n",
        "#     cnt = 1\r\n",
        "#     while j+1<l and vuv_frames[j+1] == 1:\r\n",
        "#       cnt += 1\r\n",
        "#       j += 1\r\n",
        "#     curLine = fileName+'_'+str(i*3)+'_'+str(j*3)+'='+fileName+'.fea['+str(i*3)+','+str(j*3)+']\\n'\r\n",
        "#     f.writelines(curLine)\r\n",
        "#     i = j\r\n",
        "#   i += 1\r\n",
        "# f.close()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAWGjtDhybsk"
      },
      "source": [
        "# f = open('IS1000a_Mix-Headset_25', 'r')\r\n",
        "# fw = open('IS1000a.scp', 'a')\r\n",
        "# lst = f.readlines()\r\n",
        "# # scp_vec = np.zeros((227976), dtype=int)\r\n",
        "# for i in range(13, len(lst)-2):\r\n",
        "#   if lst[i+2] == '\"sounding\"\\n':\r\n",
        "#     l = int(float(lst[i])*100)\r\n",
        "#     r = int(float(lst[i+1])*100)\r\n",
        "#     curLine = 'IS1000a_'+str(l)+'_'+str(r)+'=IS1000a.fea['+str(l)+','+str(r)+']\\n'\r\n",
        "#     fw.writelines(curLine)     \r\n",
        "# f.close()\r\n",
        "# fw.close()"
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}