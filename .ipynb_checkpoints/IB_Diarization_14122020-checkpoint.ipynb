{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cs16b108/IB_MDDP/blob/master/IB_Diarization_14122020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5jANRDmEMlV",
    "outputId": "980ff21d-5656-4cd6-9ea0-a70ec55e9e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2xzt9uJrsZF"
   },
   "outputs": [],
   "source": [
    "!pip install webrtcvad\n",
    "!pip install hmmlearn==0.2.3\n",
    "!pip install python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "h4zv8xRTrsZU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, isdir, join\n",
    "from pathlib import Path\n",
    "from sklearn import mixture\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "import pickle\n",
    "from hmmlearn import hmm\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "P9V6TB8HEEy1"
   },
   "outputs": [],
   "source": [
    "#File name and directory path\n",
    "fileName = 'ES2002c'\n",
    "drivePath = '/content/drive/My Drive/IB_Diarization/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DHCZqd13rsZW"
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "############# Read WAV and perform VAD ################\n",
    "# Already done once and files stored on google drive ##\n",
    "#######################################################\n",
    "\n",
    "import collections\n",
    "import contextlib\n",
    "import sys\n",
    "import wave\n",
    "import webrtcvad\n",
    "\n",
    "\n",
    "def read_wave(path):\n",
    "    \"\"\"Reads a .wav file.\n",
    "    Takes the path, and returns (PCM audio data, sample rate).\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        assert num_channels == 1\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        assert sample_rate in (8000, 16000, 32000, 48000)\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "        return pcm_data, sample_rate\n",
    "\n",
    "\n",
    "def write_wave(path, audio, sample_rate):\n",
    "    \"\"\"Writes a .wav file.\n",
    "    Takes path, PCM audio data, and sample rate.\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'wb')) as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(sample_rate)\n",
    "        wf.writeframes(audio)\n",
    "\n",
    "\n",
    "class Frame(object):\n",
    "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    \"\"\"Generates audio frames from PCM audio data.\n",
    "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "    the sample rate.\n",
    "    Yields Frames of the requested duration.\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "\n",
    "def vad_collector(sample_rate, frame_duration_ms,\n",
    "                  padding_duration_ms, vad, frames,vuv_frames):\n",
    "    \"\"\"Filters out non-voiced audio frames.\n",
    "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
    "    the voiced audio.\n",
    "    Uses a padded, sliding window algorithm over the audio frames.\n",
    "    When more than 90% of the frames in the window are voiced (as\n",
    "    reported by the VAD), the collector triggers and begins yielding\n",
    "    audio frames. Then the collector waits until 90% of the frames in\n",
    "    the window are unvoiced to detrigger.\n",
    "    The window is padded at the front and back to provide a small\n",
    "    amount of silence or the beginnings/endings of speech around the\n",
    "    voiced frames.\n",
    "    Arguments:\n",
    "    sample_rate - The audio sample rate, in Hz.\n",
    "    frame_duration_ms - The frame duration in milliseconds.\n",
    "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
    "    vad - An instance of webrtcvad.Vad.\n",
    "    frames - a source of audio frames (sequence or generator).\n",
    "    Returns: A generator that yields PCM audio data.\n",
    "    \"\"\"\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    # We use a deque for our sliding window/ring buffer.\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "    # NOTTRIGGERED state.\n",
    "    triggered = False\n",
    "\n",
    "    index=-1\n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "\n",
    "        index+=1\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "\n",
    "        sys.stdout.write('1' if is_speech else '0')\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "            # the ring buffer are voiced frames, then enter the\n",
    "            # TRIGGERED state.\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                sys.stdout.write('+(%s)' % (ring_buffer[0][0].timestamp,))\n",
    "                # We want to yield all the audio we see from now until\n",
    "                # we are NOTTRIGGERED, but we have to start with the\n",
    "                # audio that's already in the ring buffer.\n",
    "\n",
    "                id = 0 #we must start actually with num_padding_frames-1 and do index-- \n",
    "                for f, s in ring_buffer:\n",
    "                    voiced_frames.append(f)\n",
    "                    vuv_frames[index-id]=1\n",
    "                    id+=1\n",
    "\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            # We're in the TRIGGERED state, so collect the audio data\n",
    "            # and add it to the ring buffer.\n",
    "            voiced_frames.append(frame)\n",
    "            vuv_frames[index] = 1\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            # If more than 90% of the frames in the ring buffer are\n",
    "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "            # audio we've collected.\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
    "                triggered = False\n",
    "                yield b''.join([f.bytes for f in voiced_frames])\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if triggered:\n",
    "        sys.stdout.write('-(%s)' % (frame.timestamp + frame.duration))\n",
    "    sys.stdout.write('\\n')\n",
    "    # If we have any leftover voiced audio when we run out of input,\n",
    "    # yield it.\n",
    "    if voiced_frames:\n",
    "        yield b''.join([f.bytes for f in voiced_frames])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "na0eIG-qrsZY"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # if len(args) != 2:\n",
    "    #     sys.stderr.write(\n",
    "    #         'Usage: silenceremove.py <aggressiveness> <path to wav file>\\n')\n",
    "    #     sys.exit(1)\n",
    "    # path = '/content/amicorpus/'\n",
    "    path = join(drivePath, 'temp/'+fileName)\n",
    "    dir_list = sorted(os.listdir(path))\n",
    "    cnt = 0\n",
    "    for d in dir_list:\n",
    "      dir_name = join(path,d)\n",
    "      if not isdir(dir_name):\n",
    "        # filePath = join(path, d, 'audio', d+'.Mix-Headset.wav')\n",
    "        filePath = join(path, d)\n",
    "        # try:\n",
    "        # if 1:\n",
    "        audio, sample_rate = read_wave(filePath)\n",
    "        vad = webrtcvad.Vad(int(1))\n",
    "        frame_duration_ms = 30\n",
    "        frames = frame_generator(30, audio, sample_rate)\n",
    "        frames = list(frames)\n",
    "\n",
    "        nof_frames = 1+(len(audio)-1)/int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "        nof_frames = int(nof_frames)\n",
    "        vuv_frames = np.zeros((nof_frames,)).astype('int')\n",
    "        segments = vad_collector(sample_rate, 30, 300, vad, frames,vuv_frames)\n",
    "        print(len(frames),\" s\", sample_rate,len(audio))\n",
    "\n",
    "        # Segmenting the Voice audio and save it in list as bytes\n",
    "        concataudio = [segment for segment in segments]\n",
    "\n",
    "        joinedaudio = b\"\".join(concataudio)\n",
    "        np.save(join(drivePath,'save_files', fileName+'_vuv.npy'),vuv_frames)\n",
    "        # writePath = join('/content/drive/My Drive/IB_Diarization/temp/conv', d)\n",
    "        # Path(writePath).mkdir(parents=True, exist_ok=True)\n",
    "        # write_wave(writePath, joinedaudio, sample_rate)\n",
    "        cnt += 1\n",
    "        if(cnt == 2):\n",
    "          break\n",
    "        # except Exception as inst:\n",
    "          # print(\"Skipping: \", filePath, inst)\n",
    "    print(\"Converted: \",cnt)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "5eK9jDzPspJZ"
   },
   "outputs": [],
   "source": [
    "vuv_frames = np.load(join(drivePath,'save_files', fileName+'_vuv.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZePchLWDrsZx"
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "### Actual Code Starts from Here ###\n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5ekr76KGrsZ0"
   },
   "outputs": [],
   "source": [
    "#Define path to wav files created after VAD\n",
    "\n",
    "path = join(drivePath, 'amicorpus_non_silence/')\n",
    "\n",
    "overlap = 0.01 #10 ms window shift\n",
    "fullPath = join(path,fileName+'/audio/'+fileName+'.Mix-Headset.wav')\n",
    "(rate,sig) = wav.read(fullPath)\n",
    "mfcc_feat = mfcc(sig, rate, numcep = 19, nfilt = 26, winlen=0.03, winstep=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "YA8PWs8UgExB"
   },
   "outputs": [],
   "source": [
    "#Also read the original wav file\n",
    "path = join(drivePath, 'temp/'+fileName)\n",
    "fullPath = join(path,fileName+'.Mix-Headset.wav')\n",
    "(rate,sig) = wav.read(fullPath)\n",
    "mfcc_feat_original = mfcc(sig, rate, numcep = 19, nfilt = 26, winlen=0.03, winstep=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "tA4XNF2XxeAx"
   },
   "outputs": [],
   "source": [
    "##n is total num of frames and d is num of features per frame\n",
    "n, d = mfcc_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "NYpv2WHys4m6"
   },
   "outputs": [],
   "source": [
    "# overlap = 0.01 #10 ms window shift\n",
    "init_cluster_time = 2500 #2.5sec\n",
    "init_cluster_len = math.ceil(init_cluster_time/(overlap*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fttKNb7UtQ7z"
   },
   "outputs": [],
   "source": [
    "N = math.ceil(n/init_cluster_len)\n",
    "num_of_clusters = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_AYZWAdxN_U",
    "outputId": "755164dd-d804-4515-fa70-22cdfab67233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879 219619 250 878.476\n"
     ]
    }
   ],
   "source": [
    "print(N,n,init_cluster_len, (n/init_cluster_len) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2qzsMekx04_"
   },
   "outputs": [],
   "source": [
    "# class GMM:\n",
    "#     def __init__(self, num_of_clusters):\n",
    "#         self.num_of_clusters = num_of_clusters\n",
    "#         self.log_likelihood =[]\n",
    "#         self.LL_diff = []\n",
    "#         # self.num_of_speakers = num_of_speakers\n",
    "\n",
    "#     def gaussian_prob(self, x, mean, sigma):\n",
    "#         d = x.shape[0]\n",
    "#         p = ((2*math.pi)**(-d/2))*(np.linalg.det(sigma)**(-0.5))*np.exp(-0.5*(x-mean).reshape(d,1).T.dot(np.linalg.inv(sigma)).dot((x-mean).reshape(d,1)))\n",
    "#         return p\n",
    "\n",
    "#     def k_means(self, X):\n",
    "#         n = X.shape[0]\n",
    "#         d = X.shape[1]\n",
    "#         itr = 0\n",
    "#         #self.centroid = np.zeros((self.num_of_clusters, d), dtype = 'float64')\n",
    "#         self.centroids = X[random.sample(range(n), self.num_of_clusters)]\n",
    "#         self.cluster_assigned = np.zeros(n, dtype = int)\n",
    "#         error = 0.0\n",
    "#         while True:\n",
    "#             print(\"Now at itr - \", itr)\n",
    "#             # print(\"Centroids - \", self.centroids)\n",
    "#             for i in range(n):\n",
    "#                 f_vec = X[i]\n",
    "#                 dist = np.sqrt(np.sum((f_vec-self.centroids)**2, 1))\n",
    "#                 # print(\"Dist Shape is - \", dist.shape)\n",
    "#                 self.cluster_assigned[i] = np.argmin(dist)\n",
    "#             new_error = np.sum(np.sqrt(np.sum((X - self.centroids[self.cluster_assigned])**2, 1)))\n",
    "#             if(itr>0):\n",
    "#                 print(\"Error Difference is - \", np.abs(error-new_error))\n",
    "#             new_centroids = np.zeros((self.num_of_clusters, d), dtype = 'float64')\n",
    "#             count_of_elements = np.zeros(self.num_of_clusters, dtype = int)\n",
    "#             for i in range(n):\n",
    "#                 c_ind = self.cluster_assigned[i]\n",
    "#                 new_centroids[c_ind] += X[i]\n",
    "#                 count_of_elements[c_ind] += 1\n",
    "#             new_centroids = new_centroids/count_of_elements[:,None]\n",
    "#             if np.abs(new_error-error)<10 or np.array_equal(self.centroids, new_centroids) or itr>=5:\n",
    "#                 print(\"Breaking at itr - \", itr)\n",
    "#                 break\n",
    "#             else:\n",
    "#                 self.centroids = np.copy(new_centroids)\n",
    "#             itr += 1\n",
    "#             error = new_error\n",
    "\n",
    "#     def EM_GMM_INBUILT(self, X):\n",
    "#         N = X.shape[0]\n",
    "#         d = X.shape[1]\n",
    "#         from sklearn.mixture import GaussianMixture as GMM\n",
    "#         g = GMM(n_components=64, covariance_type = 'full', max_iter = 1)\n",
    "#         g.fit(X)\n",
    "#         print(\"Created\")\n",
    "\n",
    "#     def EM_GMM(self, X):\n",
    "#         N = X.shape[0]\n",
    "#         d = X.shape[1]\n",
    "#         self.cov_mat = np.zeros((self.num_of_clusters, d, d), dtype = 'float64')\n",
    "#         self.gamma = np.zeros((N,self.num_of_clusters), dtype = 'float64')\n",
    "#         likelihood = np.zeros((N,self.num_of_clusters), dtype = 'float64')\n",
    "#         self.pi_prob = np.zeros(self.num_of_clusters, dtype = 'float64')\n",
    "#         self.Nk = np.zeros(self.num_of_clusters, dtype = 'float64')\n",
    "#         for k in range(self.num_of_clusters):\n",
    "#             indices = (np.argwhere(self.cluster_assigned==k)).ravel()\n",
    "#             X_k = X[indices]\n",
    "#             X_k_centered = X_k - self.centroids[k]\n",
    "#             self.Nk[k] = X_k.shape[0]\n",
    "#             # print(\"Xk \",X_k.shape)\n",
    "#             # print(\"Xkc \",X_k_centered.shape)\n",
    "#             # print(\"cov mat \",self.cov_mat[k])\n",
    "#             self.cov_mat[k] = (1/self.Nk[k])*(X_k_centered.T.dot(X_k_centered))\n",
    "#         # print(self.Nk)\n",
    "#         self.pi_prob = self.Nk/N\n",
    "#         print(\"EM Begins\")\n",
    "#         itr = 1\n",
    "#         prev_log_likelihood = 0.0\n",
    "        \n",
    "#         while True:\n",
    "#             #####################################\n",
    "#             ############   E Step   #############\n",
    "#             #####################################\n",
    "#             for k in range(self.num_of_clusters):\n",
    "#                 #self.gamma[i,k] = self.gaussian_prob(X[i], self.centroids[k], self.cov_mat[k])\n",
    "#                 self.cov_mat[k] += 1e-6*np.identity(d)\n",
    "#                 likelihood[:,k] =  multivariate_normal.pdf(X, self.centroids[k], self.cov_mat[k]).ravel()\n",
    "#                 # print(\"Done \", k)\n",
    "#             # log_likelihood = np.sum(np.sum((likelihood*self.pi_prob), axis = 1))\n",
    "\n",
    "#             # for i in range(N):\n",
    "#             #     print(\"Done \",i)\n",
    "             \n",
    "#             self.gamma = likelihood*self.pi_prob\n",
    "#             self.gamma = self.gamma/(np.sum(self.gamma, axis = 1)[:,None])\n",
    "#             # print(\"E done\")\n",
    "\n",
    "#             #####################################\n",
    "#             ############   M Step   #############\n",
    "#             #####################################\n",
    "#             self.Nk = np.sum(self.gamma, axis = 0)\n",
    "#             self.pi_prob = self.Nk/N\n",
    "#             for k in range(self.num_of_clusters):\n",
    "#                 self.centroids[k] = (1/self.Nk[k])*np.sum((X*self.gamma[:,k][:,np.newaxis]), axis = 0)\n",
    "#                 X_centered = X - self.centroids[k]\n",
    "#                 self.cov_mat[k] = (1/self.Nk[k])*((X_centered*self.gamma[:,k][:,np.newaxis]).T.dot(X_centered))\n",
    "#             # print(\"M done\")\n",
    "\n",
    "#             #####################################\n",
    "#             ########   Log Likelihood   #########\n",
    "#             #####################################\n",
    "#             new_log_likelihood = np.sum(np.log(np.sum((likelihood*self.pi_prob), axis = 1)))\n",
    "#             self.log_likelihood.append(new_log_likelihood)\n",
    "#             diff_LL = np.abs(new_log_likelihood-prev_log_likelihood)\n",
    "#             self.LL_diff.append(diff_LL)\n",
    "#             print(\"Itr = \", itr, \" Current LL is - \",new_log_likelihood)\n",
    "#             print(\"Change In LL is - \",diff_LL)\n",
    "#             if(diff_LL<100 or itr>=10):\n",
    "#                 print(\"EM Finished at iteration - \", itr)\n",
    "#                 break\n",
    "#             itr += 1\n",
    "#             prev_log_likelihood = new_log_likelihood\n",
    "\n",
    "# ug = GMM(num_of_clusters)\n",
    "# ug.k_means(mfcc_feat)\n",
    "# ug.EM_GMM(mfcc_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ihYpZGpIY5WX"
   },
   "outputs": [],
   "source": [
    "def fitUnimodal(C):\n",
    "  means = []\n",
    "  covMatrices = []\n",
    "  for c in C:\n",
    "    means.append(np.mean(c, axis = 0))\n",
    "    covMatrices.append(np.cov(c.T))\n",
    "  return means, covMatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "O54Lqzff0zce"
   },
   "outputs": [],
   "source": [
    "def calc_prob(x, GaussianMeans, GaussianCovMatrices):\n",
    "  p = 0.0\n",
    "  D = x.shape[0]\n",
    "  numOfClusters = len(GaussianMeans)\n",
    "  for i in range(D):\n",
    "    s = x[i]\n",
    "    for k in range(numOfClusters):\n",
    "    #self.gamma[i,k] = self.gaussian_prob(X[i], self.centroids[k], self.cov_mat[k])\n",
    "      cov_matrix = 1e-6*np.identity(d) + GaussianCovMatrices[k]\n",
    "      # cov_matrix = \n",
    "      p =  p + ug.pi_prob[k]*multivariate_normal.pdf(s, ug.centroids[k], cov_matrix)\n",
    "  p = p/D\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6Y7msXyWJkMH"
   },
   "outputs": [],
   "source": [
    "def calcYgivenX(x, GaussianMeans, GaussianCovMatrices, i):\n",
    "  p = 0.0\n",
    "  numOfClusters = len(GaussianMeans)\n",
    "  w = 1.0/numOfClusters\n",
    "  D = x.shape[0]\n",
    "  probMat = np.zeros((D, num_of_clusters), dtype = float)\n",
    "  for i in range(num_of_clusters):\n",
    "    probMat[:,i] = multivariate_normal(x, GaussianMeans[i], GaussianCovMatrices[i])\n",
    "  p = 0.0\n",
    "  self.gamma = self.gamma/(np.sum(self.gamma, axis = 1)[:,None]) \n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "wUZLV2W5RKZo"
   },
   "outputs": [],
   "source": [
    "########################\n",
    "##### IB Algorithm #####\n",
    "########################\n",
    "\n",
    "#Init Variables\n",
    "C = np.array_split(mfcc_feat, num_of_clusters)\n",
    "GaussianMeans, GaussianCovMatrices = fitUnimodal(C)\n",
    "ClusterMapping = dict(zip(range(num_of_clusters), [[i] for i in range(num_of_clusters)]))\n",
    "beta = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "0zDI3qIlQHei"
   },
   "outputs": [],
   "source": [
    "# probC = []\n",
    "# for i in range(N):\n",
    "#   p = 0.0\n",
    "#   D = C[i].shape[0]\n",
    "#   for j in range(D):\n",
    "#     s = C[i][j]\n",
    "#     p += multivariate_normal.pdf(s, GaussianMeans[i], GaussianCovMatrices[i])\n",
    "#   p = p/D \n",
    "#   probC.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FQejbER_OfF9"
   },
   "outputs": [],
   "source": [
    "probC = (1.0/N)*np.ones(N)\n",
    "probX = probC.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfyeSjbwyHgY",
    "outputId": "3bea2464-8da2-49aa-ec6e-e48ec2e7a410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  0\n",
      "Done  100\n",
      "Done  200\n",
      "Done  300\n",
      "Done  400\n",
      "Done  500\n",
      "Done  600\n",
      "Done  700\n",
      "Done  800\n"
     ]
    }
   ],
   "source": [
    "probYgivenC = []\n",
    "probCgivenX = []\n",
    "for i in range(N):\n",
    "  temp1 = []\n",
    "  temp2 = []\n",
    "  x = C[i]\n",
    "  w = 1.0/N\n",
    "  D = x.shape[0]\n",
    "  probMat = np.zeros((D, N), dtype = float)\n",
    "  for j in range(N):\n",
    "    probMat[:,j] = multivariate_normal.pdf(x, GaussianMeans[j], GaussianCovMatrices[j]).ravel()\n",
    "  probMat = probMat/(np.sum(probMat, axis = 1)[:,None])\n",
    "  temp1 = np.mean(probMat, axis = 0)\n",
    "  for j in range(N):\n",
    "    # p = probMat[i,j]/(np.sum(probMat[i,j], axis = 1)[:,None])\n",
    "    # p = calcYgivenX(x, GaussianMeans, GaussianCovMatrices, i)\n",
    "    # temp1.append(p)\n",
    "    if j == i:\n",
    "      temp2.append(1.0)\n",
    "    else:\n",
    "      temp2.append(0.0)\n",
    "    # print(\"Done2 \",j)\n",
    "  probYgivenC.append(temp1)\n",
    "  probCgivenX.append(temp2)\n",
    "  if i%100 == 0:\n",
    "    print(\"Done \",i)\n",
    "\n",
    "# # prob_cond_y_c = np.zeros((N, N), dtype = float)\n",
    "# # prob_cond_c_x = np.zeros((N, N), dtype = float)\n",
    "# del_F = np.zeros((N, N), dtype = float)\n",
    "# for i in range(N):\n",
    "#   prob_c(i) = calc_prob(C[i], ug)\n",
    "#   for j in range(N):\n",
    "#     prob_cond_y_c[j][i] = calc_cond_prob(j, C[i], ug)\n",
    "#     if(j == i):\n",
    "#       prob_cond_c_x[j][i] = 1\n",
    "\n",
    "\n",
    "\n",
    "#Main Algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fy1GaieALLwB",
    "outputId": "3da066ee-3bda-42bb-86b7-6206240b2edc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  0\n",
      "Done  100\n",
      "Done  200\n",
      "Done  300\n",
      "Done  400\n",
      "Done  500\n",
      "Done  600\n",
      "Done  700\n",
      "Done  800\n"
     ]
    }
   ],
   "source": [
    "del_F = np.zeros((N, N), dtype = float)\n",
    "del_F[:,:] = np.inf\n",
    "probXgivenC = ((np.array(probCgivenX)*np.array(probX)).T/probC).T\n",
    "for i in range(N):\n",
    "  for j in range(i+1, N): \n",
    "    temp1 = distance.jensenshannon(np.array(probYgivenC)[:,i], np.array(probYgivenC)[:,j]) \n",
    "    temp2 = distance.jensenshannon(probXgivenC[i], probXgivenC[j]) \n",
    "    dij = temp1 - (1/beta)*temp2\n",
    "    del_F[i][j] = (probC[i] + probC[j])*dij\n",
    "    # del_F[i][j] = cal_objective_diff(C[i], C[j])\n",
    "  if i%100 == 0:\n",
    "    print(\"Done \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "3UBnkJ0-Bgl5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probYgivenC.sav'\n",
    "pickle.dump(probYgivenC, open(file_name, 'wb'))\n",
    "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probCgivenX.sav'\n",
    "pickle.dump(probCgivenX, open(file_name, 'wb'))\n",
    "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_del_F.sav'\n",
    "pickle.dump(del_F, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "lAcZYLCvL-By"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "probC = (1.0/N)*np.ones(N)\n",
    "probX = probC.copy()\n",
    "ClusterMapping = dict(zip(range(N), [[i] for i in range(N)]))\n",
    "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probYgivenC.sav'\n",
    "probYgivenC = pickle.load(open(file_name, 'rb'))\n",
    "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_probCgivenX.sav'\n",
    "# pickle.dump(probCgivenY, open(file_name, 'wb'))\n",
    "probCgivenX = pickle.load(open(file_name, 'rb'))\n",
    "file_name = '/content/drive/My Drive/IB_Diarization/'+fileName+'_del_F.sav'\n",
    "del_F = pickle.load(open(file_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "UWu50k8pX0bb"
   },
   "outputs": [],
   "source": [
    "num_of_speakers = 4\n",
    "hmm_gmm_cluster_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "nwZKVQEKHncy"
   },
   "outputs": [],
   "source": [
    "# Nif num_of_clusters == 4:\n",
    "bestClusterMapping = copy.deepcopy(ClusterMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uqki82tSBf0k",
    "outputId": "6406c4a8-197e-4f42-c00e-52ba7e89dc38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Count:  76\n",
      "Clusters Rem:  850\n",
      "NMI:  0.9760661359244392\n",
      "Time Elapsed:  2.810843535264333  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  800\n",
      "NMI:  0.9438554604666591\n",
      "Time Elapsed:  7.338755683104197  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  750\n",
      "NMI:  0.9162858771072044\n",
      "Time Elapsed:  11.577003220717112  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  700\n",
      "NMI:  0.8935194559072733\n",
      "Time Elapsed:  15.579637630780537  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  650\n",
      "NMI:  0.8594555734149203\n",
      "Time Elapsed:  19.271832331021628  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  600\n",
      "NMI:  0.8136385116507342\n",
      "Time Elapsed:  22.708418889840445  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  550\n",
      "NMI:  0.7561084391384516\n",
      "Time Elapsed:  25.899140242735545  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  500\n",
      "NMI:  0.7066188999334495\n",
      "Time Elapsed:  28.849774038791658  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  450\n",
      "NMI:  0.6330973827302447\n",
      "Time Elapsed:  31.52183584769567  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  400\n",
      "NMI:  0.589260819130265\n",
      "Time Elapsed:  33.95722486972809  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  350\n",
      "NMI:  0.5629544667807936\n",
      "Time Elapsed:  36.08588266770045  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  300\n",
      "NMI:  0.5109057884459016\n",
      "Time Elapsed:  37.94836415847143  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  250\n",
      "NMI:  0.4494478921969443\n",
      "Time Elapsed:  39.554379034042356  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  200\n",
      "NMI:  0.41257623886972117\n",
      "Time Elapsed:  40.89012269576391  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  150\n",
      "NMI:  0.37200089643631895\n",
      "Time Elapsed:  41.95282102028529  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  100\n",
      "NMI:  0.3396312244253798\n",
      "Time Elapsed:  42.76322103738785  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  50\n",
      "NMI:  0.2906383474418636\n",
      "Time Elapsed:  43.31389003594716  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  10\n",
      "NMI:  0.5617480749851617\n",
      "Time Elapsed:  43.53918634255727  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  9\n",
      "NMI:  0.5287250956031496\n",
      "Time Elapsed:  43.542074131965634  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  8\n",
      "NMI:  0.5117105488962339\n",
      "Time Elapsed:  43.545231544971465  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  7\n",
      "NMI:  0.5493949972303214\n",
      "Time Elapsed:  43.54841907024384  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  6\n",
      "NMI:  0.6499820743174654\n",
      "Time Elapsed:  43.551803437868756  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  5\n",
      "NMI:  0.6303179976870308\n",
      "Time Elapsed:  43.55487885872523  minutes\n",
      "Deep Copying dict:\n",
      "All Count:  76\n",
      "Clusters Rem:  4\n",
      "NMI:  0.37270436188789047\n",
      "Time Elapsed:  43.5579486767451  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  3\n",
      "NMI:  0.3428552540182747\n",
      "Time Elapsed:  43.56085183620453  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  2\n",
      "NMI:  0.7751342319976668\n",
      "Time Elapsed:  43.56364560524623  minutes\n",
      "All Count:  76\n",
      "Clusters Rem:  1\n",
      "NMI:  0.0731778883346223\n",
      "Time Elapsed:  43.56640756527583  minutes\n",
      "Completion Time:  43.56641583442688  minutes\n"
     ]
    }
   ],
   "source": [
    "#IB ALgo\n",
    "import time\n",
    "startTime = time.time()\n",
    "num_of_clusters = N\n",
    "# print(\"Yaha\")\n",
    "\n",
    "INFyc = 0.0\n",
    "probY = 1.0/N\n",
    "for y in range(N):\n",
    "  for c in range(num_of_clusters):\n",
    "    INFyc += probC[c]*probYgivenC[y][c]*np.log(probYgivenC[y][c]/probY)\n",
    "\n",
    "INFyx = INFyc\n",
    "NMI = INFyc/INFyx\n",
    "threshold = 0.5\n",
    "while num_of_clusters > 1:\n",
    "  # print(\"Here\")\n",
    "  # i, j = np.argwhere(del_F == np.min(del_F)).ravel()\n",
    "  mIdx = np.argmin(del_F)\n",
    "  i = mIdx//N\n",
    "  j = mIdx%N\n",
    "  # print(i,j)\n",
    "  probCr = probC[i] + probC[j]\n",
    "  del_F[:,j] = np.inf\n",
    "  del_F[j,:] = np.inf\n",
    "  # probC.pop(j)\n",
    "  ClusterMapping[i] += ClusterMapping[j]\n",
    "  ClusterMapping[j] = []\n",
    "  probYgivenC[i] = (probYgivenC[i]*probC[i] + probYgivenC[j]*probC[j])/probCr\n",
    "  probC[i] = probCr\n",
    "  probCgivenX[i] = [0 for idx in probCgivenX[i]]\n",
    "  for idx in ClusterMapping[i]:\n",
    "    probCgivenX[i][idx] = 1\n",
    "  probXgivenC = ((np.array(probCgivenX)*np.array(probX)).T/probC).T\n",
    "  for idx in range(0, i):\n",
    "    if del_F[idx,i] == np.inf:\n",
    "      continue\n",
    "    temp1 = distance.jensenshannon(np.array(probYgivenC)[:,idx], np.array(probYgivenC)[:,i]) \n",
    "    temp2 = distance.jensenshannon(probXgivenC[idx], probXgivenC[i]) \n",
    "    dij = temp1 - (1/beta)*temp2\n",
    "    del_F[idx][i] = (probC[idx] + probC[i])*dij\n",
    "  for idx in range(i+1, N):\n",
    "    if del_F[i, idx] == np.inf:\n",
    "      continue \n",
    "    temp1 = distance.jensenshannon(np.array(probYgivenC)[:,i], np.array(probYgivenC)[:,idx]) \n",
    "    temp2 = distance.jensenshannon(probXgivenC[i], probXgivenC[idx]) \n",
    "    dij = temp1 - (1/beta)*temp2\n",
    "    del_F[i][idx] = (probC[i] + probC[idx])*dij\n",
    "  num_of_clusters = num_of_clusters-1\n",
    "  \n",
    "  if num_of_clusters == num_of_speakers:\n",
    "    print(\"Deep Copying dict:\")\n",
    "    bestClusterMapping = copy.deepcopy(ClusterMapping)\n",
    "\n",
    "  INFyc = 0.0\n",
    "  for y in range(N):\n",
    "    for c in range(N):\n",
    "      if ClusterMapping[c]:\n",
    "        INFyc += probC[c]*probYgivenC[y][c]*np.log(probYgivenC[y][c]/probY)\n",
    "  NMI = INFyc/INFyx\n",
    "  if num_of_clusters%50 == 0 or num_of_clusters<=10:\n",
    "    # print(i,j)\n",
    "    # cnt = 0\n",
    "    # for idx in range(N):\n",
    "    #   if len(ClusterMapping[idx]) !=0:\n",
    "    #     cnt += len(ClusterMapping[idx])\n",
    "    #     if num_of_clusters<10:\n",
    "    #       print(idx, len(ClusterMapping[idx]))\n",
    "    # print(\"\")\n",
    "    # for idx in range(N):\n",
    "    #   if len(bestClusterMapping[idx]) !=0:\n",
    "    #     if num_of_clusters<6:\n",
    "    #       print(idx, len(bestClusterMapping[idx]))\n",
    "    print(\"All Count: \",cnt)\n",
    "    print(\"Clusters Rem: \", num_of_clusters)\n",
    "    print(\"NMI: \",NMI)\n",
    "    print(\"Time Elapsed: \", (time.time()-startTime)/60, \" minutes\")\n",
    "\n",
    "print(\"Completion Time: \", (time.time()-startTime)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ph31cevADnKY"
   },
   "outputs": [],
   "source": [
    "ClusterMapping = copy.deepcopy(bestClusterMapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "QcZ-Ojw98Y07"
   },
   "outputs": [],
   "source": [
    "#Save the results of IB Diarization\n",
    "file_name = join(drivePath,fileName+'done.sav')\n",
    "doneFile = [N, probX, probC, probYgivenC, probCgivenX, probXgivenC, ClusterMapping, GaussianMeans, GaussianCovMatrices]\n",
    "pickle.dump(doneFile, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "NBkjL9mQp_vy"
   },
   "outputs": [],
   "source": [
    "#Load the results of IB Diarization\n",
    "file_name = join(drivePath,fileName+'done.sav')\n",
    "doneFile = pickle.load(open(file_name, 'rb'))\n",
    "N = doneFile[0]\n",
    "probX= doneFile[1]\n",
    "probC= doneFile[2]\n",
    "probYgivenC= doneFile[3]\n",
    "probCgivenX= doneFile[4]\n",
    "probXgivenC= doneFile[5]\n",
    "ClusterMapping = doneFile[6]\n",
    "GaussianMeans = doneFile[7]\n",
    "GaussianCovMatrices = doneFile[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "xS310aOYdwD0"
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "#### HMM Alignment Begins Here ####\n",
    "###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "omADzRjJRWzc"
   },
   "outputs": [],
   "source": [
    "#Segregate MFCCs for each cluster for creating GMMs for HMM\n",
    "segCount = 0\n",
    "mfcc_segregated = []\n",
    "for key in ClusterMapping:\n",
    "  if(len(ClusterMapping[key])>0):\n",
    "    cur_mfcc = np.empty((0,19), dtype = float)\n",
    "    for curSeg in sorted(ClusterMapping[key]):\n",
    "      sIdx = max(0, (curSeg)*init_cluster_len)\n",
    "      eIdx = min((curSeg+1)*init_cluster_len, n)\n",
    "      cur_mfcc = np.concatenate((cur_mfcc, np.array(mfcc_feat[sIdx:eIdx])))\n",
    "    mfcc_segregated.append(cur_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "nhY3ukDqqH1g"
   },
   "outputs": [],
   "source": [
    "#Calculate parameters for initializing HMM\n",
    "gmmMeans = np.zeros((num_of_speakers, hmm_gmm_cluster_num,mfcc_feat.shape[-1]), dtype = float)\n",
    "gmmCov = np.zeros((num_of_speakers, hmm_gmm_cluster_num ,mfcc_feat.shape[-1], mfcc_feat.shape[-1]), dtype = float)\n",
    "gmmWeights = np.zeros((num_of_speakers, hmm_gmm_cluster_num ), dtype = float)\n",
    "for i, X in enumerate(mfcc_segregated):\n",
    "  gmm = mixture.GaussianMixture(n_components=hmm_gmm_cluster_num).fit(X)\n",
    "  gmmMeans[i] = gmm.means_\n",
    "  gmmCov[i] = gmm.covariances_\n",
    "  gmmWeights[i] = gmm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "yazhSYH5HCbl"
   },
   "outputs": [],
   "source": [
    "#Save GMM results\n",
    "file_name = join(drivePath, fileName+'_gmmTrained.sav')\n",
    "gmmTrained = [gmmMeans, gmmCov, gmmWeights]\n",
    "pickle.dump(gmmTrained, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "bQ2e6foNTZx1"
   },
   "outputs": [],
   "source": [
    "#Load GMM results\n",
    "file_name = join(drivePath, fileName+'_gmmTrained.sav')\n",
    "gmmTrained = pickle.load(open(file_name, 'rb'))\n",
    "gmmMeans = gmmTrained[0]\n",
    "gmmCov = gmmTrained[1]\n",
    "gmmWeights = gmmTrained[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "hXMQd4gimSjA"
   },
   "outputs": [],
   "source": [
    "#Separate the final clusters into a list\n",
    "cluster_list = []\n",
    "for i in range(N):\n",
    "  if len(ClusterMapping[i]) !=0:\n",
    "    cluster_list.append(ClusterMapping[i]);\n",
    "\n",
    "#Map Which cluster belongs to which speaker\n",
    "cluster_to_speaker = np.ones((N,))\n",
    "for sp_id, clstr in enumerate(cluster_list):\n",
    "  for c in clstr:\n",
    "    cluster_to_speaker[c] = int(sp_id)\n",
    "\n",
    "#Start probability for HMM\n",
    "start_prob=np.zeros((num_of_speakers,))\n",
    "start_prob[int(cluster_to_speaker[0])] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "q4P0fxm9x0k3"
   },
   "outputs": [],
   "source": [
    "#Calculate Transmission Probability for HMM\n",
    "transmission_prob = np.zeros((num_of_speakers,num_of_speakers), dtype = float)\n",
    "for i in range(1,N):\n",
    "  fromSpeaker = int(cluster_to_speaker[i-1])\n",
    "  toSpeaker = int(cluster_to_speaker[i])\n",
    "  transmission_prob[fromSpeaker][toSpeaker] += 1\n",
    "for i in range(num_of_speakers):\n",
    "  total = np.sum(cluster_to_speaker[:-1] == i)\n",
    "  transmission_prob[i,:] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "2gdLW_Gu3hik"
   },
   "outputs": [],
   "source": [
    "# Initializie HMM\n",
    "U_GMM_HMM =  hmm.GMMHMM(n_components = num_of_speakers,\n",
    "                        n_mix = hmm_gmm_cluster_num,\n",
    "                        covariance_type = \"full\",\n",
    "                        init_params = \"\",\n",
    "                        n_iter = 50\n",
    "                         )\n",
    "U_GMM_HMM.covars_ = gmmCov\n",
    "U_GMM_HMM.means_ = gmmMeans\n",
    "U_GMM_HMM.weights_ = gmmWeights\n",
    "U_GMM_HMM.transmat_ = transmission_prob\n",
    "U_GMM_HMM.startprob_ = start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "rrXZwq0M0kCc"
   },
   "outputs": [],
   "source": [
    "viterbiAligned = U_GMM_HMM.predict(mfcc_feat_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "10hQKhx19Z49"
   },
   "outputs": [],
   "source": [
    "#Mask each silence frame to value num_of_speakers+1\n",
    "for i in range(len(viterbiAligned)):\n",
    "  if vuv_frames[i//3] == 0:\n",
    "    viterbiAligned[i] = num_of_speakers+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zgqxt-Rp94dR"
   },
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "__OHYyJnORyA"
   },
   "outputs": [],
   "source": [
    "# finalSegments = []\n",
    "# for i in range(N):\n",
    "#   temp = viterbiAlignedSpeakers[i*init_cluster_len:min((i+1)*init_cluster_len, len(viterbiAlignedSpeakers))]\n",
    "#   x = np.bincount(temp).argmax()\n",
    "#   finalSegments.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "S9hGXt9h95gv"
   },
   "outputs": [],
   "source": [
    "#Generate final segments after viterbi alignment\n",
    "finalSegments = []\n",
    "init_cluster_timee = 2500 #2.5sec, is the min speaker segment size\n",
    "init_cluster_lenn = math.ceil(init_cluster_timee/(overlap*1000))\n",
    "NN = math.ceil(n/init_cluster_lenn)\n",
    "for i in range(NN):\n",
    "  temp = viterbiAligned[i*init_cluster_lenn:min((i+1)*init_cluster_lenn, len(viterbiAligned))]\n",
    "  x = np.bincount(temp).argmax()\n",
    "  finalSegments.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "EnS3YgMHNU1H"
   },
   "outputs": [],
   "source": [
    "#Generate RTTM file for predicted data\n",
    "def segmentToRTTM(finalSegments, init_cluster_time):\n",
    "  fileObj = open(fileName+'_predicted_2500.rttm', 'a')\n",
    "  idx = 0\n",
    "  startTime = -1\n",
    "  duration = 0\n",
    "  # speakerId\n",
    "  recName = fileName\n",
    "  channelId = '1'\n",
    "  onset = 0.000\n",
    "  duration = 0.0\n",
    "  while idx<=NN:\n",
    "    if idx == NN or finalSegments[idx] != finalSegments[idx-1]:\n",
    "      lineStr = 'SPEAKER ' + 'meeting' + ' ' + channelId + ' ' + str(onset) + ' ' + str(duration) + ' <NA> <NA> ' \\\n",
    "                    + 'speaker_'+str(int(finalSegments[idx-1])) + ' <NA> <NA>\\n'\n",
    "      if finalSegments[idx-1] != num_of_speakers+1 and duration>0.0:\n",
    "        fileObj.writelines(lineStr)\n",
    "      onset += duration\n",
    "      duration = init_cluster_time/1000\n",
    "    else:\n",
    "      duration += init_cluster_time/1000\n",
    "    idx += 1\n",
    "  fileObj.close()\n",
    "try:\n",
    "    os.remove(fileName+'_predicted_2500.rttm')\n",
    "except OSError:\n",
    "    pass\n",
    "segmentToRTTM(finalSegments, init_cluster_timee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LT1Y459bC_h7"
   },
   "outputs": [],
   "source": [
    "# f = open('actual.rttm', 'r')\n",
    "# fr = open('new_actual.rttm','a')\n",
    "# lastEnd = 0.0\n",
    "# for l in f.readlines():\n",
    "#   lst = l.split()\n",
    "#   lineStr = 'SPEAKER ' + 'meeting' + ' ' + '1' + ' ' + str(lastEnd) + ' ' + lst[4] + ' <NA> <NA> ' \\\n",
    "#                     + lst[7] + ' <NA> <NA>\\n'\n",
    "#   # lst[3] = str(lastEnd)\n",
    "#   lastEnd = lastEnd + float(lst[4])\n",
    "#   fr.writelines(lineStr)\n",
    "#   # break\n",
    "# f.close()\n",
    "# fr.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Nw_EMmdWwDm9"
   },
   "outputs": [],
   "source": [
    "# f = open(fileName+'.scp', 'a')\n",
    "# l = len(vuv_frames)\n",
    "# i = 0\n",
    "# while i<l:\n",
    "#   if vuv_frames[i] == 1:\n",
    "#     j = i\n",
    "#     cnt = 1\n",
    "#     while j+1<l and vuv_frames[j+1] == 1:\n",
    "#       cnt += 1\n",
    "#       j += 1\n",
    "#     curLine = fileName+'_'+str(i*3)+'_'+str(j*3)+'='+fileName+'.fea['+str(i*3)+','+str(j*3)+']\\n'\n",
    "#     f.writelines(curLine)\n",
    "#     i = j\n",
    "#   i += 1\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RAWGjtDhybsk"
   },
   "outputs": [],
   "source": [
    "# f = open('IS1000a_Mix-Headset_25', 'r')\n",
    "# fw = open('IS1000a.scp', 'a')\n",
    "# lst = f.readlines()\n",
    "# # scp_vec = np.zeros((227976), dtype=int)\n",
    "# for i in range(13, len(lst)-2):\n",
    "#   if lst[i+2] == '\"sounding\"\\n':\n",
    "#     l = int(float(lst[i])*100)\n",
    "#     r = int(float(lst[i+1])*100)\n",
    "#     curLine = 'IS1000a_'+str(l)+'_'+str(r)+'=IS1000a.fea['+str(l)+','+str(r)+']\\n'\n",
    "#     fw.writelines(curLine)     \n",
    "# f.close()\n",
    "# fw.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IB_Diarization_14122020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
